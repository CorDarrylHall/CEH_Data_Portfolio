[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\nShow the code\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "posts/Billboard's Top 100 (2000)/bill.html",
    "href": "posts/Billboard's Top 100 (2000)/bill.html",
    "title": "Billboard Top 100’s in 2000",
    "section": "",
    "text": "This analysis focuses on the Billboard Top 100 chart from the year 2000, particularly examining the #1 hit singles that stayed at the top position the longest. Using data manipulation and visualization techniques in R, we can gain insights into the music trends of that year and identify the most dominant songs and artists."
  },
  {
    "objectID": "posts/Billboard's Top 100 (2000)/bill.html#libraries-and-data-preparation",
    "href": "posts/Billboard's Top 100 (2000)/bill.html#libraries-and-data-preparation",
    "title": "Billboard Top 100’s in 2000",
    "section": "Libraries and Data Preparation",
    "text": "Libraries and Data Preparation\nWe start by loading the necessary R libraries for data manipulation, visualization, and analysis.\n\n\nShow the code\nlibrary(tidyverse)   # For data manipulation and visualization\nlibrary(lubridate)   # For date manipulation\nlibrary(skimr)       # For summarizing data\nlibrary(survival)    # For survival analysis (not used in this example)\nlibrary(survminer)   # For visualizing survival analysis (not used in this example)\nlibrary(flextable)   # For creating flexible tables\nlibrary(DT)          # For interactive tables\n\n\nThe billboard dataset is reshaped to gather weekly rankings into a long format, making it easier to filter and analyze the data.\n\n\nShow the code\n# Load the dataset\ndata(\"billboard\", package = \"tidyverse\") # Ensure you have the billboard dataset loaded\nds = billboard\n\n# Reshape the data from wide to long format\nds = billboard %>% gather(key = week, value = rank, wk1:wk76)\n\n# Convert week column to numeric and ensure rank is numeric\nds$week = as.numeric(gsub(\"wk\", \"\", ds$week))\nds$rank = as.numeric(ds$rank)"
  },
  {
    "objectID": "posts/Billboard's Top 100 (2000)/bill.html#filtering-and-summarizing-data",
    "href": "posts/Billboard's Top 100 (2000)/bill.html#filtering-and-summarizing-data",
    "title": "Billboard Top 100’s in 2000",
    "section": "Filtering and Summarizing Data",
    "text": "Filtering and Summarizing Data\nWe filter the dataset to include only the rows where the song was ranked #1. Then, we group by artist and track to count the number of weeks each song stayed at the top position.\n\n\nShow the code\n# Filter for #1 ranked songs and summarize the duration at #1\nds = ds %>%\n  filter(rank == 1) %>%\n  group_by(artist, track) %>%\n  summarize(weeksAtNumberOne = n()) %>%\n  arrange(desc(weeksAtNumberOne))\n\n# Display the summarized data as a flextable\nas_flextable(ds)\n\n\n\nartisttrackweeksAtNumberOnecharactercharacterintegerDestiny's ChildIndependent Women Pa...11SantanaMaria, Maria10Aguilera, ChristinaCome On Over Baby (A...4MadonnaMusic4Savage GardenI Knew I Loved You4Destiny's ChildSay My Name3Iglesias, EnriqueBe With You3JanetDoesn't Really Matte...3Aguilera, ChristinaWhat A Girl Wants2LonestarAmazed2n: 17\n\n\nWe use the flextable library to create a flexible and visually appealing table that displays the artists and tracks along with the number of weeks they stayed at #1."
  },
  {
    "objectID": "posts/Billboard's Top 100 (2000)/bill.html#additional-visualizations",
    "href": "posts/Billboard's Top 100 (2000)/bill.html#additional-visualizations",
    "title": "Billboard Top 100’s in 2000",
    "section": "Additional Visualizations",
    "text": "Additional Visualizations\n\n\nShow the code\n# Summarize total weeks at #1 for each artist\nartist_summary <- ds %>%\n  group_by(artist) %>%\n  summarize(totalWeeksAtNumberOne = sum(weeksAtNumberOne)) %>%\n  arrange(desc(totalWeeksAtNumberOne))\n\n# Bar chart of top artists by total weeks at #1\nggplot(artist_summary, aes(x = reorder(artist, totalWeeksAtNumberOne), y = totalWeeksAtNumberOne)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top Artists by Total Weeks at #1 in 2000\", x = \"Artist\", y = \"Total Weeks at #1\") +\n  theme_minimal()\n\n\n\n\n\n\n\nShow the code\n# Histogram of weeks at #1\nggplot(ds, aes(x = weeksAtNumberOne)) +\n  geom_histogram(binwidth = 1, fill = \"darkorange\", color = \"black\") +\n  labs(title = \"Distribution of Weeks at #1\", x = \"Weeks at #1\", y = \"Count of Songs\") +\n  theme_minimal()\n\n\n\n\n\n\n\nShow the code\n# Interactive data table\ndatatable(ds, options = list(pageLength = 10, autoWidth = TRUE),\n          caption = 'Number of Weeks Each Song Stayed at #1 in 2000')"
  },
  {
    "objectID": "posts/Billboard's Top 100 (2000)/bill.html#conclusion",
    "href": "posts/Billboard's Top 100 (2000)/bill.html#conclusion",
    "title": "Billboard Top 100’s in 2000",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis provides a clear view of the songs that dominated the Billboard Top 100 charts in the year 2000. By identifying the tracks that remained at #1 the longest, we can better understand the music trends and preferences of that time. This type of analysis can be expanded to include other years or additional chart metrics to gain further insights into the evolution of popular music."
  },
  {
    "objectID": "posts/Human Resource Analytics Dashboard/powerbilhr.html",
    "href": "posts/Human Resource Analytics Dashboard/powerbilhr.html",
    "title": "Human Resources Analytics Dashboard",
    "section": "",
    "text": "Presented here is a real-life Power BI Dashboard that I crafted for a previous organization, offering a glimpse into the intricate web of key performance indicators (KPIs) and metrics vital for organizational success. Divided into distinct sections, each segment of the dashboard delivers unique insights tailored to the organization’s overarching objectives.\nIn essence, the Power BI Dashboard serves as a beacon guiding decision-makers through the labyrinth of organizational performance. By facilitating real-time monitoring, trend identification, and data-driven decision-making, it fosters a holistic approach to performance management.\nThe accompanying visuals further enrich the narrative, offering succinct summaries and insights into critical facets of organizational dynamics. The HR summary, for instance, delves into workforce demographics, retention trends, and diversity metrics, serving as a compass for inclusive talent management strategies.\nThe HR summary offers a detailed overview of the workforce demographics, including ethnicity, gender, and retention trends. It is a vital tool for assessing diversity, inclusion efforts, and identifying areas for improvement.\nDemographic Analysis: Displays the composition of the workforce by ethnicity and gender.\nRetention Trends: Showcases turnover rates, average tenure, and reasons for employee departures."
  },
  {
    "objectID": "posts/Human Resource Analytics Dashboard/powerbilhr.html#terminations",
    "href": "posts/Human Resource Analytics Dashboard/powerbilhr.html#terminations",
    "title": "Human Resources Analytics Dashboard",
    "section": "Terminations",
    "text": "Terminations\nLikewise, the Terminations Summary lays bare the landscape of employee departures, providing valuable insights into turnover patterns and contributing factors. Complemented by legal compliance considerations, it serves as a cornerstone for optimizing HR processes.\nThis summary provides insights into employee terminations, categorizing them by reasons such as voluntary resignations, involuntary dismissals, and retirements.\nTermination Reasons: Helps identify patterns in employee departures.\nHR Process Optimization: Offers insights for improving employee engagement and performance management processes."
  },
  {
    "objectID": "posts/Human Resource Analytics Dashboard/powerbilhr.html#talent-summary",
    "href": "posts/Human Resource Analytics Dashboard/powerbilhr.html#talent-summary",
    "title": "Human Resources Analytics Dashboard",
    "section": "Talent Summary",
    "text": "Talent Summary\nFinally, the Talent Summary emerges as a beacon for talent management, leveraging data insights to inform strategic decisions on acquisition, development, and retention. Together, these summaries form an arsenal of actionable insights, empowering organizations to navigate the complexities of talent dynamics with precision and foresight.\nThe Talent Summary provides an overview of the organization’s talent management, focusing on acquisition, development, and retention.\nTalent Acquisition: Analyzes the effectiveness of recruitment strategies.\nEmployee Development: Tracks progress and impact of training programs.\nRetention Strategies: Evaluates the success of initiatives aimed at retaining top talent."
  },
  {
    "objectID": "posts/Human Resource Analytics Dashboard/powerbilhr.html#conclusion",
    "href": "posts/Human Resource Analytics Dashboard/powerbilhr.html#conclusion",
    "title": "Human Resources Analytics Dashboard",
    "section": "Conclusion",
    "text": "Conclusion\nThe Human Resources Analytics Dashboard is an indispensable tool for organizational leaders, providing comprehensive insights into financial performance, sales trends, customer satisfaction, and workforce dynamics. By facilitating data-driven decision-making, it empowers the organization to enhance performance management, drive growth, and foster a culture of continuous improvement."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Wine /Wine.html",
    "href": "posts/Wine /Wine.html",
    "title": "Wine & Feature Engineering",
    "section": "",
    "text": "In this analysis, we will generate a comprehensive set of 10 features derived from the wine dataset, which includes the points feature. Through the process of feature engineering, we will select and transform variables to create meaningful features that enhance the predictive power of our model. This involves consolidating similar categories using functions like fct_lump and ensuring that our dataset is clean by removing any rows with missing values. Additionally, we will transform the price variable into its logarithmic form (log(price)) to stabilize the variance and normalize the distribution, facilitating a more effective linear regression model.\n\n\nShow the code\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(fastDummies)\nwine = read_rds(\"/Users/Shared/Data 505/wine.rds\")"
  },
  {
    "objectID": "posts/Wine /Wine.html#feature-engineering",
    "href": "posts/Wine /Wine.html#feature-engineering",
    "title": "Wine & Feature Engineering",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nSummary\nIn this section, we will create a total of 10 features, including the points feature, from the wine dataset. We will also remove all rows that contain any missing values to ensure the data is clean and complete. Finally, we will transform the price into its logarithmic form and ensure that only the log-transformed price (log(price)) and the selected features remain in the final dataframe, which we will call wino.\n\n\nShow the code\nwino <- wine %>%\n  mutate(lprice = log(price)) %>%\n  mutate(country = fct_lump(country, 5),\n         taster_name = fct_lump(taster_name, 5),\n         variety = fct_lump(variety, 5),\n         winery = fct_lump(winery, 5),\n         region_1 = fct_lump(region_1, 5),\n         province = fct_lump(province, 5),\n         designation = fct_lump(designation, 5)) %>%\n  select(lprice, points, country, taster_name, variety, winery, region_1, province, designation) %>%\n  drop_na()"
  },
  {
    "objectID": "posts/Wine /Wine.html#model-training-with-caret",
    "href": "posts/Wine /Wine.html#model-training-with-caret",
    "title": "Wine & Feature Engineering",
    "section": "Model Training with Caret",
    "text": "Model Training with Caret\nSummary\nThis section focuses on using the Caret library to partition the wino dataframe into an 80% training set and a 20% test set. We will perform a linear regression with bootstrap resampling and report the Root Mean Squared Error (RMSE) for the model on the test set. The bootstrap method involves resampling with replacement, which helps in estimating the accuracy of the model.\n\n\nShow the code\nset.seed(504)\nwine_index <- createDataPartition(wino$lprice, p = 0.8, list = FALSE)\nwino_tr <- wino[wine_index, ]\nwino_te <- wino[-wine_index, ]\n\ncontrol <- trainControl(method = \"boot\", number = 5)\nm1 <- train(lprice ~ ., \n            data = wino_tr, \n            method = \"lm\",\n            trControl = control)\n\nprint(m1$resample)\n\n\n       RMSE  Rsquared       MAE  Resample\n1 0.4716720 0.4639796 0.3669938 Resample1\n2 0.4735139 0.4531511 0.3691902 Resample2\n3 0.4682085 0.4532082 0.3652813 Resample3\n4 0.4724564 0.4578321 0.3690610 Resample4\n5 0.4718331 0.4647390 0.3683953 Resample5\n\n\nShow the code\nwine_pred <- predict(m1, wino_te)\npostResample(pred = wine_pred, obs = wino_te$lprice)\n\n\n     RMSE  Rsquared       MAE \n0.4716279 0.4453607 0.3677621"
  },
  {
    "objectID": "posts/Wine /Wine.html#variable-selection",
    "href": "posts/Wine /Wine.html#variable-selection",
    "title": "Wine & Feature Engineering",
    "section": "Variable Selection",
    "text": "Variable Selection\nSummary\nIn this section, we will identify and visualize the importance of the selected features in our model. The goal is to understand which features have the most significant impact on predicting the log-transformed price of the wine.\n\n\nShow the code\nimportance <- varImp(m1, scale = TRUE)\nplot(importance)"
  },
  {
    "objectID": "posts/Wine /Wine.html#data-partition",
    "href": "posts/Wine /Wine.html#data-partition",
    "title": "Wine & Feature Engineering",
    "section": "Data Partition",
    "text": "Data Partition\nSummary\nTo ensure reproducibility, we will set the seed to 504 before partitioning the data into training and test sets. We will aim to achieve an RMSE on the test data of less than 0.47 for 1 point, less than 0.46 for 2 points, or less than 0.45 for 3 points. This ensures that the model’s performance is both robust and reproducible.\n\n\nShow the code\nset.seed(504)\nwine_pred <- predict(m1, wino_te)\npostResample(pred = wine_pred, obs = wino_te$lprice)\n\n\n     RMSE  Rsquared       MAE \n0.4716279 0.4453607 0.3677621"
  },
  {
    "objectID": "posts/Wine /Wine.html#conclusion",
    "href": "posts/Wine /Wine.html#conclusion",
    "title": "Wine & Feature Engineering",
    "section": "Conclusion:",
    "text": "Conclusion:\nThis presentation demonstrated the process of feature engineering, model training using the Caret library, and evaluating the model’s performance. Key steps included creating features, handling missing data, partitioning the dataset, and assessing the model using RMSE. The importance of reproducibility in data partitioning and model evaluation was emphasized through the use of set.seed."
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "",
    "text": "This study aims to predict job roles accurately by analyzing the relationship between sleep patterns, job classification, and other relevant factors. By utilizing sleep-related data and demographics in a model, the researchers seek to optimize workforce planning and develop personalized workplace strategies"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#problem-statement",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#problem-statement",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Problem Statement",
    "text": "Problem Statement\nThe research aims to improve human resource management strategies and personalized workplace interventions by accurately predicting job classifications through the analysis of sleep patterns and other health-related factors, exploring the relationship between sleep, stress, and job characteristics."
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#research-questions",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#research-questions",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Research Questions",
    "text": "Research Questions\nTo what extent do sleep patterns, along with other factors, correlate with a person’s job classification?\nHow accurately can a classification model predict a person’s job based on their sleep patterns and other relevant factors?\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\n\nprint(\"Libraries imported successfully!\")\n\nimport os\n\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\n\n# Check if the file exists\nif os.path.exists(file_path):\n    print(\"File exists, proceeding to load.\")\n    try:\n        sleep = pd.read_csv(file_path)\n        print(\"File loaded successfully.\")\n        print(sleep.head())  # Print the first few rows to confirm the data is loaded\n    except Exception as e:\n        print(f\"An error occurred while loading the file: {e}\")\nelse:\n    print(\"File does not exist, please check the file path.\")\n    \nsleep = pd.read_csv(file_path)\n\nimport pandas as pd\nimport seaborn as sns\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\nsleep.info()\n\nsleep.describe()\n\n# Select some columns that might have a correlation\nnumerical_data = sleep[['Age', 'Sleep Duration','Quality of Sleep','Physical Activity Level', 'Stress Level','Heart Rate', 'Daily Steps']]\ncorrelation_matrix = numerical_data.corr()\n\n# Generate the heatmap and use a new color sheme\nax = sns.heatmap(correlation_matrix, annot=True, cmap=\"PuOr\");\nfig = ax.get_figure()"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#correlation-of-numeric-variables",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#correlation-of-numeric-variables",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Correlation of Numeric Variables",
    "text": "Correlation of Numeric Variables\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\n# Select some columns that might have a correlation\nnumerical_data = sleep[['Age', 'Sleep Duration','Quality of Sleep','Physical Activity Level', 'Stress Level','Heart Rate', 'Daily Steps']]\ncorrelation_matrix = numerical_data.corr()\n\n# Generate the heatmap and use a new color sheme\nplt.figure(figsize=(10, 8))\nax = sns.heatmap(correlation_matrix, annot=True, cmap=\"PuOr\");\nfig = ax.get_figure()\n\nplt.title('Correlation Matrix of Selected Features')\nplt.show()"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#occupation-by-the-numbers",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#occupation-by-the-numbers",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Occupation By The Numbers",
    "text": "Occupation By The Numbers\nimport pandas as pd\nimport seaborn as sns\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\nsleep.Occupation.unique()\n\nsleep.groupby('Occupation').agg(\n    num_occ=('Occupation', 'size')\n).reset_index()\nimport pandas as pd\nimport seaborn as sns\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\ndef occupation_group(row):\n    if row[\"Occupation\"] in [\"Salesperson\", \"Sales Representative\", \"Manager\"]:\n        return \"Sales\"\n    elif row[\"Occupation\"] in [\"Software Engineer\", \"Scientist\", \"Accountant\"]:\n        return \"STEM\"\n    elif row[\"Occupation\"] in [\"Doctor\", \"Nurse\"]:\n        return \"Medical\"\n    else:\n        return row[\"Occupation\"]\n\nsleep[\"Occupation_Group\"] = sleep.apply(occupation_group, axis=1)"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#look-at-distribution-based-on-age",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#look-at-distribution-based-on-age",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Look at distribution based on Age",
    "text": "Look at distribution based on Age\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\n# Look at the distribution based on Age\nfig, ax = plt.subplots(figsize=(10, 5))\nax.hist(sleep['Age'], bins=15)\nax.set_title('Distribution of Ages')\nax.set_xlabel('Age')\nax.set_ylabel('Frequency')\nplt.show()"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#stress-level",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#stress-level",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Stress Level",
    "text": "Stress Level\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\nsleep.groupby('Stress Level').agg(\n    num_individuals=('Stress Level', 'size')\n).reset_index()"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#sleep-duration",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#sleep-duration",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Sleep Duration",
    "text": "Sleep Duration\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\n# Look at the distribution based on Age\nfig, ax = plt.subplots(figsize=(10, 5))\nax.hist(sleep['Sleep Duration'], bins=20)\nax.set_title('Distribution of Sleep Duration')\nax.set_xlabel('Sleep Duration')\nax.set_ylabel('Frequency')\nplt.show()\n#Figure out who is 8.5?"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#quality-of-sleep",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#quality-of-sleep",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Quality of Sleep",
    "text": "Quality of Sleep\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\nsleep.groupby('Quality of Sleep').agg(\n    num_individuals=('Quality of Sleep', 'size')\n).reset_index()"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#gender",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#gender",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Gender",
    "text": "Gender\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\nsleep.groupby('Gender').agg(\n    num_occ=('Gender', 'size')\n).reset_index()\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\ngrouped_data = sleep.groupby(['Gender', 'Occupation']).size().reset_index(name='n')\n\n# Pivot the data to get 'Gender' as columns and 'Occupation' as index\npivot_data = grouped_data.pivot(index='Occupation', columns='Gender', values='n').fillna(0)\n\n# Rename the columns and reset the index\npivot_data.columns.name = None\npivot_data = pivot_data.reset_index()\n\npivot_data\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\ngrouped_data = sleep.groupby(['Stress Level', 'Occupation']).size().reset_index(name='n')\n\n# Pivot the data to get 'Gender' as columns and 'Occupation' as index\npivot_data = grouped_data.pivot(index='Occupation', columns='Stress Level', values='n').fillna(0)\n\n# Rename the columns and reset the index\npivot_data.columns.name = None\npivot_data = pivot_data.reset_index()\n\npivot_data\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n# Pivot the data to get 'Gender' as columns and 'Occupation' as index\ngrouped_data = sleep.groupby(['Stress Level', 'Occupation']).size().reset_index(name='n')\npivot_data = grouped_data.pivot(index='Occupation', columns='Stress Level', values='n').fillna(0)\npivot_data.columns.name = None\npivot_data = pivot_data.reset_index()\n\npivot_data\n# Create the heatmap using seaborn\nsns.heatmap(pivot_data.set_index('Occupation'), annot=True, cmap='YlGnBu', fmt='g')\n\n# Set the title and labels\nplt.title(\"Stress Level vs Occupation Heatmap\")\nplt.xlabel(\"Stress Level\")\nplt.ylabel(\"Occupation\")\n\n# Display the heatmap\nplt.show()\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n# Pivot the data to get 'Gender' as columns and 'Occupation' as index\n\ngrouped_data = sleep.groupby(['Quality of Sleep', 'Stress Level']).size().reset_index(name='Total Counts')\n\n\n# Pivot the data to create a heatmap\npivot_data = grouped_data.pivot(index='Stress Level', columns='Quality of Sleep', values='Total Counts').fillna(0)\n\n# Create the heatmap using seaborn\nplt.figure(figsize=(10, 6))\nsns.heatmap(pivot_data, annot=True, cmap='YlGnBu', fmt='g')\nplt.xlabel('Quality of Sleep')\nplt.ylabel('Stress Level')\nplt.title('Heatmap of Quality of Sleep vs. Stress Level')\nplt.show()"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#machine-learning",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#machine-learning",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Machine Learning",
    "text": "Machine Learning\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# Load the dataset\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\n# Separate features and target variable\nx = sleep.drop(columns=['Person ID', 'Sleep Disorder', 'Blood Pressure'])\ny = sleep['Sleep Disorder']\n\n# Inspect the data for NaNs\nprint(\"Checking for NaNs in the dataset:\")\nprint(x.isnull().sum())\n\n# Handle missing values\n# Impute numerical columns with mean\nnumerical_cols = x.select_dtypes(include=['float64', 'int64']).columns\nimputer_num = SimpleImputer(strategy='mean')\nx[numerical_cols] = imputer_num.fit_transform(x[numerical_cols])\n\n# Impute categorical columns with most frequent value\ncategorical_cols = ['Gender', 'Occupation', 'BMI Category', 'Stress Level']\nimputer_cat = SimpleImputer(strategy='most_frequent')\nx[categorical_cols] = imputer_cat.fit_transform(x[categorical_cols])\n\n# Recheck for any remaining NaN values\nprint(\"Rechecking for NaNs after imputation:\")\nprint(x.isnull().sum())\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\nfor col in categorical_cols:\n    x[col] = label_encoder.fit_transform(x[col])\n\n# Ensure there are no NaNs before proceeding\nif x.isnull().values.any():\n    raise ValueError(\"Data still contains NaN values after handling missing data.\")\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# One-hot encode the categorical columns\nencoder = OneHotEncoder()\nx_train_encoded = encoder.fit_transform(x_train[categorical_cols])\nx_test_encoded = encoder.transform(x_test[categorical_cols])\n\n# Combine the one-hot encoded features with other numerical features\nx_train_final = np.hstack((x_train_encoded.toarray(), x_train.drop(columns=categorical_cols).values))\nx_test_final = np.hstack((x_test_encoded.toarray(), x_test.drop(columns=categorical_cols).values))\n\n# Fit the RandomForestClassifier and make predictions\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(x_train_final, y_train)\ny_pred = rf_classifier.predict(x_test_final)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy of the random forest classifier:\", accuracy)\n\n# Display the confusion matrix\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, labels=rf_classifier.classes_, display_labels=rf_classifier.classes_, cmap=\"YlGnBu\")\nplt.show()"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#random-forrest-vs.-svm-classifier",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#random-forrest-vs.-svm-classifier",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Random Forrest vs. SVM Classifier",
    "text": "Random Forrest vs. SVM Classifier\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n#Load the dataset\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\n# Assuming 'sleep' is your DataFrame\nx = sleep.drop(columns=['Person ID', 'Sleep Disorder', 'Blood Pressure'])\ny = sleep['Sleep Disorder']\n\nlabel_encoder = LabelEncoder()\ncategorical_cols = ['Gender', 'Occupation', 'BMI Category', 'Stress Level']\nfor col in categorical_cols:\n    x[col] = label_encoder.fit_transform(x[col])\n\n# One-hot encode the categorical columns (if needed)\nencoder = OneHotEncoder()\nx_encoded = encoder.fit_transform(x[categorical_cols])\nx_final = np.hstack((x_encoded.toarray(), x.drop(columns=categorical_cols).values))\n\nx_train, x_test, y_train, y_test = train_test_split(x_final, y, test_size=0.2, random_state=42)\n\n# RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(x_train, y_train)\ny_pred_rf = rf_classifier.predict(x_test)\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\n\n# SVM Classifier\nsvm_classifier = SVC(kernel='linear', random_state=42)\nsvm_classifier.fit(x_train, y_train)\ny_pred_svm = svm_classifier.predict(x_test)\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\n\nprint(\"Accuracy of the random forest classifier:\", accuracy_rf)\nprint(\"Accuracy of the SVM classifier:\", accuracy_svm)\n\nConfusionMatrixDisplay.from_predictions(y_test, y_pred_svm, display_labels=svm_classifier.classes_, cmap=\"YlGnBu\")\nplt.show()"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#clustering",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#clustering",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Clustering",
    "text": "Clustering\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load the dataset\nfile_path = '/Users/Shared/Python/Sleep_health_and_lifestyle_dataset.csv'\nsleep = pd.read_csv(file_path)\n\n# Assuming 'sleep' is your DataFrame\n# Split 'Blood Pressure' into 'Systolic' and 'Diastolic'\nbp_split = sleep['Blood Pressure'].str.split('/', expand=True)\nsleep['Systolic'] = pd.to_numeric(bp_split[0], errors='coerce')\nsleep['Diastolic'] = pd.to_numeric(bp_split[1], errors='coerce')\n\n# Drop the original 'Blood Pressure' column\nx = sleep.drop(columns=['Person ID', 'Sleep Disorder', 'Gender', 'Blood Pressure'])\ny = sleep['Sleep Disorder']\n\n# Encode categorical variables\nlabel_encoder = LabelEncoder()\ncategorical_cols = ['Occupation', 'BMI Category', 'Stress Level']\nfor col in categorical_cols:\n    x[col] = label_encoder.fit_transform(x[col])\n\n# Normalize the data\nscaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)\n\n# Perform K-means clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans_clusters = kmeans.fit_predict(x_scaled)\n\n# Visualize the clusters\nplt.scatter(x_scaled[:, 0], x_scaled[:, 1], c=kmeans_clusters, cmap='viridis', marker='o')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('K-means Clustering')\nplt.show()"
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#summary",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#summary",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Summary",
    "text": "Summary\nThis study delved into the relationship between sleep patterns, job classification, and other relevant factors to accurately predict job roles. By leveraging data analytics and machine learning techniques, we aimed to optimize workforce planning and develop personalized workplace strategies.\nKey Insights\n1.Correlation Analysis: Identified significant correlations between sleep duration, stress levels, and job classifications.\n2.Distribution Analysis: Provided insights into the age and sleep duration distributions among different occupations.\n3.Heatmaps Highlighted the relationship between stress levels and sleep quality across various occupations.\n4.Machine Learning Models: Demonstrated that Random Forest and SVM classifiers could predict sleep disorders with high accuracy based on sleep-related data.\n5.Clustering: K-means clustering revealed distinct groups within the data, indicating potential subgroups with unique sleep and job characteristics."
  },
  {
    "objectID": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#conclusions",
    "href": "posts/Sleep and Employment: Predicting Job Classification through Data Analytics/Sleeping.html#conclusions",
    "title": "Sleep and Employment: Predicting Job Classification through Data Analytics",
    "section": "Conclusions",
    "text": "Conclusions\n1.Predictive Power: Sleep patterns, stress levels, and demographic data can effectively predict job classifications.\n2.Model Performance: Random Forest classifiers showed promising accuracy, outperforming SVM in this context.\n3.Workforce Planning: These insights can inform HR strategies, promoting better sleep health and productivity among employees.\n4.Future Research: Further studies can explore additional factors such as dietary habits and mental health for a comprehensive analysis.\nThis comprehensive analysis underscores the critical role of sleep patterns in predicting job classifications, offering valuable insights for both researchers and HR professionals. By integrating these findings into workforce management strategies, organizations can foster a healthier, more productive work environment."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CorDarryl Hall’s Data Portfolio",
    "section": "",
    "text": "Billboard Top 100’s in 2000\n\n\n\nTime Series Analysis\n\n\nData Analysis\n\n\nData Science\n\n\nData Visualization\n\n\nTrend Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman Resources Analytics Dashboard\n\n\n\nPower BI\n\n\nData Visualization\n\n\nKPI Analysis\n\n\nHR Analytics\n\n\nEmployee Retention\n\n\nTurnover Analysis\n\n\nData-Driven Decision Making\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHarlow Malloc\n\n\nMay 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSleep and Employment: Predicting Job Classification through Data Analytics\n\n\n\nPython\n\n\nData Analysis\n\n\nData Science\n\n\nData Visualization\n\n\nTrend Analysis\n\n\nMachine Learning\n\n\nRandom Forest Classification\n\n\nSVM Classifier\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nTristan O’Malley\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWine & Feature Engineering\n\n\n\nMachine Learning\n\n\nData Analysis\n\n\nData Science\n\n\nFeature Engineering\n\n\nPredictive Modeling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]